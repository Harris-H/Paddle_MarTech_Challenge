{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 项目说明\n",
    "\n",
    "### 1 任务说明\n",
    "智能营销工具可以帮助商家预测用户购买的行为，根据品牌商家的历史订单数据，构建并训练预测模型，用以预估用户群体在规定时间内产生购买行为的概率。\n",
    "该模型可应用于各种电商数据分析，不仅可以帮助商家基于平台流量，进行商品售卖、支付，还可以通过MarTech技术更精准地锁定核心用户，对用户的购买行为进行预测。\n",
    "\n",
    "### 2 任务类型\n",
    "根据任务说明以及实际数据，将该任务定义为数据挖掘下的二分类任务；既可以运用时间序列模型进行分析，也可以按照常规的分类任务的思路解决；本项目基于baseline实现，使用多层感知机模型完成此任务。\n",
    "\n",
    "### 3 解决方案\n",
    "基于比赛提供的baseline实现，在其上通过变动模型结构和二分类阈值以提升评估指标值。\n",
    "\n",
    "### 4 总结改进\n",
    "根据赛题重点，合理有效地处理数据集的各类特征是完成分类任务的关键之处。\n",
    "本项目只是使用较为初级的多层感知机网络执行分类任务，项目可改进的地方包含但不限于：\n",
    "\n",
    "1. 进一步细化特征处理办法，深化特征工程有关工作；\n",
    "2. 改进或换用预测模型结构，可以尝试使用现代深度学习框架内更为先进的神经网络模型；\n",
    "3. 更换任务思路，采用传统机器学习项目中时间序列分析的相关思路与模型解决该问题。\n",
    "\n",
    "### 5 飞桨使用\n",
    "在使用paddlepaddle进行深度学习时，注重理论课程与实践应用的合理结合； 一方面，强调通过资料与视频课程领会框架的基本使用； 另一方面，需要结合具体应用（如参加飞桨的各类竞赛）熟练掌握数据预处理、模型构建、模型训练、模型调优与应用等深度学习各阶段操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1 数据导入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1.1 数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\r\n",
    "import re\r\n",
    "import gc\r\n",
    "import time\r\n",
    "import random\r\n",
    "import numpy as np  \r\n",
    "import pandas as pd \r\n",
    "import seaborn as sns\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "from scipy.stats import skew \r\n",
    "from scipy.special import boxcox1p\r\n",
    "from scipy.stats import boxcox_normmax\r\n",
    "from itertools import product\r\n",
    "import calendar\r\n",
    "import datetime as dt\r\n",
    "from datetime import datetime, date, timedelta\r\n",
    "\r\n",
    "\r\n",
    "# 读入数据\r\n",
    "PATH = './data/data19383/'\r\n",
    "train = pd.read_csv(PATH + 'train.csv')\r\n",
    "test  = pd.read_csv(PATH + 'submission.csv').set_index('customer_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1.2 内存优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# @from: https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65/code\r\n",
    "# @liscense: Apache 2.0\r\n",
    "# @author: weijian\r\n",
    "\r\n",
    "def reduce_mem_usage(props):\r\n",
    "    # 计算当前内存\r\n",
    "    start_mem_usg = props.memory_usage().sum() / 1024 ** 2\r\n",
    "    print(\"Memory usage of the dataframe is :\", start_mem_usg, \"MB\")\r\n",
    "    \r\n",
    "    # 哪些列包含空值，空值用-999填充。why：因为np.nan当做float处理\r\n",
    "    NAlist = []\r\n",
    "    for col in props.columns:\r\n",
    "        # 这里只过滤了objectd格式，如果你的代码中还包含其他类型，请一并过滤\r\n",
    "        if (props[col].dtypes != object):\r\n",
    "            \r\n",
    "            # print(\"**************************\")\r\n",
    "            # print(\"columns: \", col)\r\n",
    "            # print(\"dtype before\", props[col].dtype)\r\n",
    "            \r\n",
    "            # 判断是否是int类型\r\n",
    "            isInt = False\r\n",
    "            mmax = props[col].max()\r\n",
    "            mmin = props[col].min()\r\n",
    "            \r\n",
    "            # Integer does not support NA, therefore Na needs to be filled\r\n",
    "            if not np.isfinite(props[col]).all():\r\n",
    "                NAlist.append(col)\r\n",
    "                props[col].fillna(-999, inplace=True) # 用-999填充\r\n",
    "                \r\n",
    "            # test if column can be converted to an integer\r\n",
    "            asint = props[col].fillna(0).astype(np.int64)\r\n",
    "            result = np.fabs(props[col] - asint)\r\n",
    "            result = result.sum()\r\n",
    "            if result < 0.01: # 绝对误差和小于0.01认为可以转换的，要根据task修改\r\n",
    "                isInt = True\r\n",
    "            \r\n",
    "            # make interger / unsigned Integer datatypes\r\n",
    "            if isInt:\r\n",
    "                if mmin >= 0: # 最小值大于0，转换成无符号整型\r\n",
    "                    if mmax <= 255:\r\n",
    "                        props[col] = props[col].astype(np.uint8)\r\n",
    "                    elif mmax <= 65535:\r\n",
    "                        props[col] = props[col].astype(np.uint16)\r\n",
    "                    elif mmax <= 4294967295:\r\n",
    "                        props[col] = props[col].astype(np.uint32)\r\n",
    "                    else:\r\n",
    "                        props[col] = props[col].astype(np.uint64)\r\n",
    "                else: # 转换成有符号整型\r\n",
    "                    if mmin > np.iinfo(np.int8).min and mmax < np.iinfo(np.int8).max:\r\n",
    "                        props[col] = props[col].astype(np.int8)\r\n",
    "                    elif mmin > np.iinfo(np.int16).min and mmax < np.iinfo(np.int16).max:\r\n",
    "                        props[col] = props[col].astype(np.int16)\r\n",
    "                    elif mmin > np.iinfo(np.int32).min and mmax < np.iinfo(np.int32).max:\r\n",
    "                        props[col] = props[col].astype(np.int32)\r\n",
    "                    elif mmin > np.iinfo(np.int64).min and mmax < np.iinfo(np.int64).max:\r\n",
    "                        props[col] = props[col].astype(np.int64)  \r\n",
    "            else: # 注意：这里对于float都转换成float16，需要根据你的情况自己更改\r\n",
    "                props[col] = props[col].astype(np.float16)\r\n",
    "            \r\n",
    "            # print(\"dtype after\", props[col].dtype)\r\n",
    "            # print(\"********************************\")\r\n",
    "    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\r\n",
    "    mem_usg = props.memory_usage().sum() / 1024**2 \r\n",
    "    print(\"Memory usage is: \",mem_usg,\" MB\")\r\n",
    "    print(\"This is \",100*mem_usg/start_mem_usg,\"% of the initial size\")\r\n",
    "    return props, NAlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.1 字段处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 设置id字段的数据类型 (6)\r\n",
    "train['order_detail_id'] = train['order_detail_id'].astype(np.uint32)\r\n",
    "train['order_id'] = train['order_id'].astype(np.uint32)\r\n",
    "train['customer_id'] = train['customer_id'].astype(np.uint32)\r\n",
    "train['goods_id'] = train['goods_id'].astype(np.uint32)\r\n",
    "train['goods_class_id'] = train['goods_class_id'].astype(np.uint32)\r\n",
    "train['member_id'] = train['member_id'].astype(np.uint32)\r\n",
    "\r\n",
    "# 设置状态字段的数据类型并将空值置为0 (10)\r\n",
    "train['order_status'] = train['order_status'].astype(np.uint8)\r\n",
    "train['goods_has_discount'] = train['goods_has_discount'].astype(np.uint8)\r\n",
    "train[\"is_member_actived\"].fillna(0, inplace=True)\r\n",
    "train[\"is_member_actived\"]=train[\"is_member_actived\"].astype(np.int8)\r\n",
    "train[\"member_status\"].fillna(0, inplace=True)\r\n",
    "train[\"member_status\"]=train[\"member_status\"].astype(np.int8)\r\n",
    "train[\"customer_gender\"].fillna(0, inplace=True)\r\n",
    "train[\"customer_gender\"]=train[\"customer_gender\"].astype(np.int8)\r\n",
    "train['is_customer_rate'] = train['is_customer_rate'].astype(np.uint8)\r\n",
    "train['order_detail_status'] = train['order_detail_status'].astype(np.uint8)\r\n",
    "\r\n",
    "# 设置日期字段的格式 (3)\r\n",
    "train['goods_list_time']=pd.to_datetime(train['goods_list_time'],format=\"%Y-%m-%d\")\r\n",
    "train['order_pay_time']=pd.to_datetime(train['order_pay_time'],format=\"%Y-%m-%d\")\r\n",
    "train['goods_delist_time']=pd.to_datetime(train['goods_delist_time'],format=\"%Y-%m-%d\")\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.2 构造特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 2.2.1 每日付款金额\n",
    "\n",
    "注意，成功交易的客户数量不等于全部客户数量，说明有相当一部分客户虽然下过单，但是没有成功的订单，那么这些客户自然应当算在训练集之外。\n",
    "数据合并时，由于`test.csv`中，已经设置了默认0值，只需要和训练后的预测标签做一个`left join`就可以了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df = train[train.order_pay_time>'2013-02-01'] # 按订单支付时间抽取样本\r\n",
    "df['date'] = pd.DatetimeIndex(df['order_pay_time']).date  # 增加data一列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_payment = df[['customer_id','date','order_total_payment']]\r\n",
    "df_payment = df_payment.groupby(['date','customer_id']).agg({'order_total_payment': ['sum']})\r\n",
    "df_payment.columns = ['day_total_payment']  # 重设列名\r\n",
    "df_payment.reset_index(inplace=True)\r\n",
    "\r\n",
    "df_payment = df_payment.set_index(\r\n",
    "    [\"customer_id\", \"date\"])[[\"day_total_payment\"]].unstack(level=-1).fillna(0)\r\n",
    "df_payment.columns = df_payment.columns.get_level_values(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 2.2.2 每日购买数量\r\n",
    "该场景每天都有成交记录，这样就不需要考虑生成完整时间段填充的问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_goods = df[['customer_id','date','order_total_num']]\r\n",
    "df_goods = df_goods.groupby(['date','customer_id']).agg({'order_total_num': ['sum']})\r\n",
    "df_goods.columns = ['day_total_num']\r\n",
    "df_goods.reset_index(inplace=True)\r\n",
    "df_goods = df_goods.set_index(\r\n",
    "    [\"customer_id\", \"date\"])[[\"day_total_num\"]].unstack(level=-1).fillna(0)\r\n",
    "df_goods.columns = df_goods.columns.get_level_values(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.3 数据准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1. 构造dataset这里有个取巧的地方，因为要预测的9月份除了开学季以外不是非常特殊的月份，因此主要考虑近期的因素，数据集的开始时间也是2月1日，尽量避免了双十一、元旦假期的影响，当然春节假期继续保留。同时，构造数据集的时候保留了customer_id，主要为了与其它特征做整合。\n",
    "2. 通过一个函数整合付款金额和商品数量的时间滑窗，主要是因为分开做到时候合并占用内存更大，并且函数最后在返回值处做了内存优化，用时间代价尽可能避免内存溢出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 2.3.1 数据准备函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 这是一个时间滑窗函数，获得dt之前minus天以来periods的dataframe，以便进一步计算\r\n",
    "def get_timespan(df, dt, minus, periods, freq='D'):\r\n",
    "    return df[pd.date_range(dt - timedelta(days=minus), periods=periods, freq=freq)]\r\n",
    "\r\n",
    "    \r\n",
    "def prepare_dataset(df_payment, df_goods, t2018, is_train=True):\r\n",
    "    X = {}\r\n",
    "    # 整合用户id\r\n",
    "    tmp = df_payment.reset_index()\r\n",
    "    X['customer_id'] = tmp['customer_id']\r\n",
    "    # 消费特征\r\n",
    "    print('Preparing payment feature...')\r\n",
    "    for i in [14,30,60,91]:\r\n",
    "        tmp = get_timespan(df_payment, t2018, i, i)\r\n",
    "        # X['diff_%s_mean' % i] = tmp_1.diff(axis=1).mean(axis=1).values\r\n",
    "        X['mean_%s_decay' % i] = (tmp * np.power(0.9, np.arange(i)[::-1])).sum(axis=1).values\r\n",
    "        # X['mean_%s' % i] = tmp_1.mean(axis=1).values\r\n",
    "        # X['median_%s' % i] = tmp.median(axis=1).values\r\n",
    "        # X['min_%s' % i] = tmp_1.min(axis=1).values\r\n",
    "        X['max_%s' % i] = tmp.max(axis=1).values\r\n",
    "        # X['std_%s' % i] = tmp_1.std(axis=1).values\r\n",
    "        X['sum_%s' % i] = tmp.sum(axis=1).values\r\n",
    "    for i in [14,30,60,91]:\r\n",
    "        tmp = get_timespan(df_payment, t2018 + timedelta(days=-7), i, i)\r\n",
    "        X['mean_%s_decay_2' % i] = (tmp * np.power(0.9, np.arange(i)[::-1])).sum(axis=1).values\r\n",
    "        # X['mean_%s_2' % i] = tmp_2.mean(axis=1).values\r\n",
    "        # X['median_%s_2' % i] = tmp.median(axis=1).values\r\n",
    "        # X['min_%s_2' % i] = tmp_2.min(axis=1).values\r\n",
    "        X['max_%s_2' % i] = tmp.max(axis=1).values\r\n",
    "        # X['std_%s_2' % i] = tmp_2.std(axis=1).values\r\n",
    "    for i in [14,30,60,91]:\r\n",
    "        tmp = get_timespan(df_payment, t2018, i, i)\r\n",
    "        X['has_sales_days_in_last_%s' % i] = (tmp != 0).sum(axis=1).values\r\n",
    "        X['last_has_sales_day_in_last_%s' % i] = i - ((tmp != 0) * np.arange(i)).max(axis=1).values\r\n",
    "        X['first_has_sales_day_in_last_%s' % i] = ((tmp != 0) * np.arange(i, 0, -1)).max(axis=1).values\r\n",
    "\r\n",
    "    # 对此处进行微调，主要考虑近期因素\r\n",
    "    for i in range(1, 4):\r\n",
    "        X['day_%s_2018' % i] = get_timespan(df_payment, t2018, i*30, 30).sum(axis=1).values\r\n",
    "    # 商品数量特征，这里故意把时间和消费特征错开，提高时间滑窗的覆盖面\r\n",
    "    print('Preparing num feature...')\r\n",
    "    for i in [21,49,84]:\r\n",
    "            tmp = get_timespan(df_goods, t2018, i, i)\r\n",
    "            # X['goods_diff_%s_mean' % i] = tmp_1.diff(axis=1).mean(axis=1).values\r\n",
    "            # X['goods_mean_%s_decay' % i] = (tmp_1 * np.power(0.9, np.arange(i)[::-1])).sum(axis=1).values\r\n",
    "            X['goods_mean_%s' % i] = tmp.mean(axis=1).values\r\n",
    "            # X['goods_median_%s' % i] = tmp.median(axis=1).values\r\n",
    "            # X['goods_min_%s' % i] = tmp_1.min(axis=1).values\r\n",
    "            X['goods_max_%s' % i] = tmp.max(axis=1).values\r\n",
    "            # X['goods_std_%s' % i] = tmp_1.std(axis=1).values\r\n",
    "            X['goods_sum_%s' % i] = tmp.sum(axis=1).values\r\n",
    "    for i in [21,49,84]:    \r\n",
    "            tmp = get_timespan(df_goods, t2018 + timedelta(weeks=-1), i, i)\r\n",
    "            # X['goods_diff_%s_mean_2' % i] = tmp_2.diff(axis=1).mean(axis=1).values\r\n",
    "            # X['goods_mean_%s_decay_2' % i] = (tmp_2 * np.power(0.9, np.arange(i)[::-1])).sum(axis=1).values\r\n",
    "            X['goods_mean_%s_2' % i] = tmp.mean(axis=1).values\r\n",
    "            # X['goods_median_%s_2' % i] = tmp.median(axis=1).values\r\n",
    "            # X['goods_min_%s_2' % i] = tmp_2.min(axis=1).values\r\n",
    "            X['goods_max_%s_2' % i] = tmp.max(axis=1).values\r\n",
    "            X['goods_sum_%s_2' % i] = tmp.sum(axis=1).values\r\n",
    "    for i in [21,49,84]:    \r\n",
    "            tmp = get_timespan(df_goods, t2018, i, i)\r\n",
    "            X['goods_has_sales_days_in_last_%s' % i] = (tmp > 0).sum(axis=1).values\r\n",
    "            X['goods_last_has_sales_day_in_last_%s' % i] = i - ((tmp > 0) * np.arange(i)).max(axis=1).values\r\n",
    "            X['goods_first_has_sales_day_in_last_%s' % i] = ((tmp > 0) * np.arange(i, 0, -1)).max(axis=1).values\r\n",
    "\r\n",
    "\r\n",
    "    # 对此处进行微调，主要考虑近期因素\r\n",
    "    for i in range(1, 4):\r\n",
    "        X['goods_day_%s_2018' % i] = get_timespan(df_goods, t2018, i*28, 28).sum(axis=1).values\r\n",
    "\r\n",
    "    X = pd.DataFrame(X)\r\n",
    "    \r\n",
    "    reduce_mem_usage(X)\r\n",
    "    \r\n",
    "    if is_train:\r\n",
    "        # 这样转换之后，打标签直接用numpy切片就可以了\r\n",
    "        # 当然这里前提是确认付款总额没有负数的问题\r\n",
    "        X['label'] = df_goods[pd.date_range(t2018, periods=30)].max(axis=1).values\r\n",
    "        X['label'][X['label'] > 0] = 1\r\n",
    "        return X\r\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 2.3.2 训练数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing payment feature...\n",
      "Preparing num feature...\n",
      "Memory usage of the dataframe is : 345.16221618652344 MB\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  73.87003993988037  MB\n",
      "This is  21.401542948710667 % of the initial size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:80: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing payment feature...\n",
      "Preparing num feature...\n",
      "Memory usage of the dataframe is : 345.16221618652344 MB\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  73.87003993988037  MB\n",
      "This is  21.401542948710667 % of the initial size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:80: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing payment feature...\n",
      "Preparing num feature...\n",
      "Memory usage of the dataframe is : 345.16221618652344 MB\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  73.87003993988037  MB\n",
      "This is  21.401542948710667 % of the initial size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:80: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing payment feature...\n",
      "Preparing num feature...\n",
      "Memory usage of the dataframe is : 345.16221618652344 MB\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  73.87003993988037  MB\n",
      "This is  21.401542948710667 % of the initial size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/ipykernel_launcher.py:80: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "num_days = 4\r\n",
    "t2017 = date(2013, 7, 1)\r\n",
    "X_l, y_l = [], []\r\n",
    "for i in range(num_days):\r\n",
    "    delta = timedelta(days=7 * i)\r\n",
    "    X_tmp = prepare_dataset(df_payment, df_goods, t2017 + delta)\r\n",
    "    X_tmp = pd.concat([X_tmp], axis=1)\r\n",
    "\r\n",
    "    X_l.append(X_tmp)\r\n",
    "\r\n",
    "X_train = pd.concat(X_l, axis=0)\r\n",
    "del X_l, y_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 2.3.3 测试数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing payment feature...\n",
      "Preparing num feature...\n",
      "Memory usage of the dataframe is : 345.16221618652344 MB\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  73.87003993988037  MB\n",
      "This is  21.401542948710667 % of the initial size\n"
     ]
    }
   ],
   "source": [
    "X_test = prepare_dataset(df_payment, df_goods, date(2013, 9, 1), is_train=False)\r\n",
    "X_test = pd.concat([X_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 2.3.4 数据保存记录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train.to_csv('./X_train.csv')\r\n",
    "X_test.to_csv('./X_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.4 选取特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 读取经过预处理的数据\r\n",
    "X_train_read = pd.read_csv('X_train.csv')\r\n",
    "X_train_read.drop(['Unnamed: 0','customer_id'], inplace=True, axis=1)\r\n",
    "\r\n",
    "X_test_read = pd.read_csv('X_test.csv')\r\n",
    "X_test_read.drop(['Unnamed: 0','customer_id'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 选择待输入的特征列\n",
    "\n",
    "input_train_features = [  \n",
    "                     'has_sales_days_in_last_14',\n",
    "                     'last_has_sales_day_in_last_14', \n",
    "                     'first_has_sales_day_in_last_14',\n",
    "\n",
    "                     'has_sales_days_in_last_30',\n",
    "                     'last_has_sales_day_in_last_30',\n",
    "                     'first_has_sales_day_in_last_30', \n",
    "\n",
    "                     'has_sales_days_in_last_60',\n",
    "                     'last_has_sales_day_in_last_60', \n",
    "                     'first_has_sales_day_in_last_60',\n",
    "\n",
    "                     'has_sales_days_in_last_91',\n",
    "                     'last_has_sales_day_in_last_91',\n",
    "\n",
    "                     'goods_mean_21', 'goods_max_21', 'goods_sum_21',\n",
    "                     'goods_mean_49', 'goods_max_49', 'goods_sum_49',\n",
    "                     'goods_mean_84', 'goods_max_84', 'goods_sum_84', \n",
    "                     'goods_mean_21_2', 'goods_max_21_2','goods_sum_21_2', \n",
    "                     'goods_mean_49_2', 'goods_max_49_2', 'goods_sum_49_2',\n",
    "                     'goods_mean_84_2', 'goods_max_84_2', 'goods_sum_84_2',\n",
    "\n",
    "                     'goods_has_sales_days_in_last_21',\n",
    "                     'goods_last_has_sales_day_in_last_21',\n",
    "                     'goods_first_has_sales_day_in_last_21',\n",
    "\n",
    "                     'goods_has_sales_days_in_last_49',\n",
    "                     'goods_last_has_sales_day_in_last_49',\n",
    "                     'goods_first_has_sales_day_in_last_49',\n",
    "\n",
    "                     'goods_has_sales_days_in_last_84',\n",
    "                     'goods_last_has_sales_day_in_last_84',\n",
    "                     'goods_first_has_sales_day_in_last_84', \n",
    "\n",
    "                     'goods_day_1_2018','goods_day_2_2018', 'goods_day_3_2018',\n",
    "\n",
    "                     'label'\n",
    "                ]\n",
    "\n",
    "input_test_features = input_train_features.copy()\n",
    "del input_test_features[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = X_train_read[input_train_features]  \r\n",
    "X_test = X_test_read[input_test_features]\r\n",
    "\r\n",
    "X_train = X_train.reset_index(drop=True)\r\n",
    "X_test = X_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 数据归一化\r\n",
    "X_train = (X_train - X_train.min()) / (X_train.max() - X_train.min())\r\n",
    "X_test = (X_test - X_test.min()) / (X_test.max() - X_test.min())\r\n",
    "\r\n",
    "# 由于标签也被归一化，需要还原回去\r\n",
    "X_train['label'][X_train['label'] > 0] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.5 数据划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set done.\n",
      "test set done.\n"
     ]
    }
   ],
   "source": [
    "# 训练数据与验证数据\r\n",
    "def load_data(df,istrain):\r\n",
    "    \r\n",
    "    data = df\r\n",
    "\r\n",
    "    feature_num = len(data.columns)\r\n",
    "\r\n",
    "    # 将原始数据进行Reshape\r\n",
    "    data = np.array(data)\r\n",
    "    data = data.reshape([-1, feature_num])\r\n",
    "    \r\n",
    "    # 划分比例\r\n",
    "    if istrain == True:\r\n",
    "        ratio = 0.8\r\n",
    "        offset = int(data.shape[0] * ratio)\r\n",
    "        training_data = data[:offset]\r\n",
    "        test_data = data[offset:]\r\n",
    "    else:\r\n",
    "        training_data = data\r\n",
    "        test_data = None\r\n",
    "\r\n",
    "    return training_data, test_data\r\n",
    "\r\n",
    "\r\n",
    "# 加载处理后的数据\r\n",
    "training_data, dev_data = load_data(X_train,True)\r\n",
    "print('train set done.')\r\n",
    "test_data, none = load_data(X_test,False)\r\n",
    "print('test set done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3 搭建网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.1 结构定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\r\n",
    "import paddle.fluid as fluid\r\n",
    "import paddle.fluid.dygraph as dygraph\r\n",
    "from paddle.fluid.dygraph import Linear\r\n",
    "\r\n",
    "\r\n",
    "class Regressor(fluid.dygraph.Layer):\r\n",
    "\r\n",
    "    def __init__(self, name_scope):\r\n",
    "        super(Regressor, self).__init__(name_scope)\r\n",
    "        name_scope = self.full_name()\r\n",
    "\r\n",
    "        self.fc1 = Linear(input_dim=len(input_test_features), output_dim=512, act='relu') \r\n",
    "        self.fc2 = Linear(input_dim=512, output_dim=128, act='relu') \r\n",
    "        self.fc3 = Linear(input_dim=128, output_dim=64, act='relu')\r\n",
    "        self.fc4 = Linear(input_dim=64, output_dim=1, act='sigmoid')\r\n",
    "    \r\n",
    "    def forward(self, inputs):\r\n",
    "        x = self.fc1(inputs)\r\n",
    "        x = self.fc2(x)\r\n",
    "        x = self.fc3(x)\r\n",
    "        x = self.fc4(x)\r\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.2 模型配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0801 11:21:51.053849    98 device_context.cc:252] Please NOTE: device: 0, CUDA Capability: 70, Driver API Version: 11.2, Runtime API Version: 9.0\n",
      "W0801 11:21:51.058857    98 device_context.cc:260] device: 0, cuDNN Version: 7.6.\n"
     ]
    }
   ],
   "source": [
    "with fluid.dygraph.guard():\r\n",
    "\r\n",
    "    # 声明定义好的线性回归模型\r\n",
    "    model = Regressor(\"Regressor\")\r\n",
    "\r\n",
    "    # 开启模型训练模式\r\n",
    "    model.train()\r\n",
    "\r\n",
    "    # 定义优化算法，这里使用Adam Optimizer\r\n",
    "    opt = fluid.optimizer.Adam(learning_rate=0.00005, parameter_list=model.parameters())\r\n",
    "\r\n",
    "    \r\n",
    "# 针对类别不平衡问题自定义损失函数\r\n",
    "def wce_loss(pred, label, w=48, epsilon=1e-05): # w 是给到 y=1 类别的权重，越大越重视\r\n",
    "    label = fluid.layers.clip(label, epsilon, 1-epsilon)\r\n",
    "    pred = fluid.layers.clip(pred, epsilon, 1-epsilon)\r\n",
    "\r\n",
    "    loss = -1 * (w * label * fluid.layers.log(pred) + (1 - label) * fluid.layers.log(1 - pred))\r\n",
    "    loss = fluid.layers.reduce_mean(loss)\r\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.3 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 0, loss is: [5.5129666]\n",
      "epoch: 0, iter: 20, loss is: [4.6771607]\n",
      "epoch: 0, iter: 40, loss is: [4.1181993]\n",
      "epoch: 0, iter: 60, loss is: [3.6884623]\n",
      "epoch: 0, iter: 80, loss is: [3.33423]\n",
      "epoch: 0, iter: 100, loss is: [3.2013075]\n",
      "epoch: 0, iter: 120, loss is: [2.9309716]\n",
      "epoch: 0, iter: 140, loss is: [2.7775202]\n",
      "epoch: 0, iter: 160, loss is: [2.703033]\n",
      "epoch: 0, iter: 180, loss is: [2.6392713]\n",
      "epoch: 0, iter: 200, loss is: [2.5794866]\n",
      "epoch: 0, iter: 220, loss is: [2.5288477]\n",
      "epoch: 0, iter: 240, loss is: [2.5392087]\n",
      "epoch: 0, iter: 260, loss is: [2.5116413]\n",
      "epoch: 0, iter: 280, loss is: [2.5610123]\n",
      "epoch: 0, iter: 300, loss is: [2.4655511]\n",
      "epoch: 1, iter: 0, loss is: [2.4796627]\n",
      "epoch: 1, iter: 20, loss is: [2.4942076]\n",
      "epoch: 1, iter: 40, loss is: [2.4632065]\n",
      "epoch: 1, iter: 60, loss is: [2.455991]\n",
      "epoch: 1, iter: 80, loss is: [2.458689]\n",
      "epoch: 1, iter: 100, loss is: [2.4525628]\n",
      "epoch: 1, iter: 120, loss is: [2.4503186]\n",
      "epoch: 1, iter: 140, loss is: [2.4641035]\n",
      "epoch: 1, iter: 160, loss is: [2.457128]\n",
      "epoch: 1, iter: 180, loss is: [2.4404447]\n",
      "epoch: 1, iter: 200, loss is: [2.4002542]\n",
      "epoch: 1, iter: 220, loss is: [2.420582]\n",
      "epoch: 1, iter: 240, loss is: [2.4203484]\n",
      "epoch: 1, iter: 260, loss is: [2.3988187]\n",
      "epoch: 1, iter: 280, loss is: [2.463694]\n",
      "epoch: 1, iter: 300, loss is: [2.4199078]\n",
      "epoch: 2, iter: 0, loss is: [2.4126768]\n",
      "epoch: 2, iter: 20, loss is: [2.4781814]\n",
      "epoch: 2, iter: 40, loss is: [2.4387193]\n",
      "epoch: 2, iter: 60, loss is: [2.4516892]\n",
      "epoch: 2, iter: 80, loss is: [2.382669]\n",
      "epoch: 2, iter: 100, loss is: [2.402965]\n",
      "epoch: 2, iter: 120, loss is: [2.4379609]\n",
      "epoch: 2, iter: 140, loss is: [2.4015245]\n",
      "epoch: 2, iter: 160, loss is: [2.4356048]\n",
      "epoch: 2, iter: 180, loss is: [2.4148812]\n",
      "epoch: 2, iter: 200, loss is: [2.3654408]\n",
      "epoch: 2, iter: 220, loss is: [2.4280245]\n",
      "epoch: 2, iter: 240, loss is: [2.4289367]\n",
      "epoch: 2, iter: 260, loss is: [2.4277463]\n",
      "epoch: 2, iter: 280, loss is: [2.4280798]\n",
      "epoch: 2, iter: 300, loss is: [2.378251]\n",
      "epoch: 3, iter: 0, loss is: [2.4540553]\n",
      "epoch: 3, iter: 20, loss is: [2.3688025]\n",
      "epoch: 3, iter: 40, loss is: [2.358952]\n",
      "epoch: 3, iter: 60, loss is: [2.364342]\n",
      "epoch: 3, iter: 80, loss is: [2.3820393]\n",
      "epoch: 3, iter: 100, loss is: [2.405742]\n",
      "epoch: 3, iter: 120, loss is: [2.40363]\n",
      "epoch: 3, iter: 140, loss is: [2.4420555]\n",
      "epoch: 3, iter: 160, loss is: [2.3817856]\n",
      "epoch: 3, iter: 180, loss is: [2.4185827]\n",
      "epoch: 3, iter: 200, loss is: [2.3876295]\n",
      "epoch: 3, iter: 220, loss is: [2.450245]\n",
      "epoch: 3, iter: 240, loss is: [2.4056768]\n",
      "epoch: 3, iter: 260, loss is: [2.5031636]\n",
      "epoch: 3, iter: 280, loss is: [2.420545]\n",
      "epoch: 3, iter: 300, loss is: [2.4148192]\n",
      "epoch: 4, iter: 0, loss is: [2.385283]\n",
      "epoch: 4, iter: 20, loss is: [2.428985]\n",
      "epoch: 4, iter: 40, loss is: [2.4125957]\n",
      "epoch: 4, iter: 60, loss is: [2.455558]\n",
      "epoch: 4, iter: 80, loss is: [2.4608495]\n",
      "epoch: 4, iter: 100, loss is: [2.3982184]\n",
      "epoch: 4, iter: 120, loss is: [2.3760848]\n",
      "epoch: 4, iter: 140, loss is: [2.4148026]\n",
      "epoch: 4, iter: 160, loss is: [2.4317055]\n",
      "epoch: 4, iter: 180, loss is: [2.4355156]\n",
      "epoch: 4, iter: 200, loss is: [2.5004532]\n",
      "epoch: 4, iter: 220, loss is: [2.4001606]\n",
      "epoch: 4, iter: 240, loss is: [2.3944597]\n",
      "epoch: 4, iter: 260, loss is: [2.476412]\n",
      "epoch: 4, iter: 280, loss is: [2.4486384]\n",
      "epoch: 4, iter: 300, loss is: [2.449814]\n",
      "epoch: 5, iter: 0, loss is: [2.3838081]\n",
      "epoch: 5, iter: 20, loss is: [2.448578]\n",
      "epoch: 5, iter: 40, loss is: [2.4354331]\n",
      "epoch: 5, iter: 60, loss is: [2.435924]\n",
      "epoch: 5, iter: 80, loss is: [2.3925438]\n",
      "epoch: 5, iter: 100, loss is: [2.3763502]\n",
      "epoch: 5, iter: 120, loss is: [2.386419]\n",
      "epoch: 5, iter: 140, loss is: [2.4095838]\n",
      "epoch: 5, iter: 160, loss is: [2.400336]\n",
      "epoch: 5, iter: 180, loss is: [2.3557222]\n",
      "epoch: 5, iter: 200, loss is: [2.4626634]\n",
      "epoch: 5, iter: 220, loss is: [2.403498]\n",
      "epoch: 5, iter: 240, loss is: [2.4098208]\n",
      "epoch: 5, iter: 260, loss is: [2.373919]\n",
      "epoch: 5, iter: 280, loss is: [2.4129133]\n",
      "epoch: 5, iter: 300, loss is: [2.41314]\n",
      "epoch: 6, iter: 0, loss is: [2.4428449]\n",
      "epoch: 6, iter: 20, loss is: [2.4535186]\n",
      "epoch: 6, iter: 40, loss is: [2.4430609]\n",
      "epoch: 6, iter: 60, loss is: [2.4261167]\n",
      "epoch: 6, iter: 80, loss is: [2.416152]\n",
      "epoch: 6, iter: 100, loss is: [2.3953125]\n",
      "epoch: 6, iter: 120, loss is: [2.3902512]\n",
      "epoch: 6, iter: 140, loss is: [2.3910885]\n",
      "epoch: 6, iter: 160, loss is: [2.4231482]\n",
      "epoch: 6, iter: 180, loss is: [2.4179304]\n",
      "epoch: 6, iter: 200, loss is: [2.4105837]\n",
      "epoch: 6, iter: 220, loss is: [2.4727201]\n",
      "epoch: 6, iter: 240, loss is: [2.375596]\n",
      "epoch: 6, iter: 260, loss is: [2.4656272]\n",
      "epoch: 6, iter: 280, loss is: [2.4109461]\n",
      "epoch: 6, iter: 300, loss is: [2.4051266]\n",
      "epoch: 7, iter: 0, loss is: [2.4086056]\n",
      "epoch: 7, iter: 20, loss is: [2.397527]\n",
      "epoch: 7, iter: 40, loss is: [2.4659758]\n",
      "epoch: 7, iter: 60, loss is: [2.4169583]\n",
      "epoch: 7, iter: 80, loss is: [2.3996248]\n",
      "epoch: 7, iter: 100, loss is: [2.4101126]\n",
      "epoch: 7, iter: 120, loss is: [2.4445806]\n",
      "epoch: 7, iter: 140, loss is: [2.3792553]\n",
      "epoch: 7, iter: 160, loss is: [2.4306211]\n",
      "epoch: 7, iter: 180, loss is: [2.4445195]\n",
      "epoch: 7, iter: 200, loss is: [2.3932815]\n",
      "epoch: 7, iter: 220, loss is: [2.4002616]\n",
      "epoch: 7, iter: 240, loss is: [2.3907516]\n",
      "epoch: 7, iter: 260, loss is: [2.3924224]\n",
      "epoch: 7, iter: 280, loss is: [2.40055]\n",
      "epoch: 7, iter: 300, loss is: [2.3176205]\n",
      "epoch: 8, iter: 0, loss is: [2.405095]\n",
      "epoch: 8, iter: 20, loss is: [2.4007394]\n",
      "epoch: 8, iter: 40, loss is: [2.414398]\n",
      "epoch: 8, iter: 60, loss is: [2.3906884]\n",
      "epoch: 8, iter: 80, loss is: [2.4077027]\n",
      "epoch: 8, iter: 100, loss is: [2.3982131]\n",
      "epoch: 8, iter: 120, loss is: [2.405851]\n",
      "epoch: 8, iter: 140, loss is: [2.4404035]\n",
      "epoch: 8, iter: 160, loss is: [2.4392762]\n",
      "epoch: 8, iter: 180, loss is: [2.430345]\n",
      "epoch: 8, iter: 200, loss is: [2.3886585]\n",
      "epoch: 8, iter: 220, loss is: [2.410711]\n",
      "epoch: 8, iter: 240, loss is: [2.3958716]\n",
      "epoch: 8, iter: 260, loss is: [2.4105659]\n",
      "epoch: 8, iter: 280, loss is: [2.3739743]\n",
      "epoch: 8, iter: 300, loss is: [2.4623973]\n",
      "epoch: 9, iter: 0, loss is: [2.4093387]\n",
      "epoch: 9, iter: 20, loss is: [2.3624568]\n",
      "epoch: 9, iter: 40, loss is: [2.4077027]\n",
      "epoch: 9, iter: 60, loss is: [2.3616343]\n",
      "epoch: 9, iter: 80, loss is: [2.3792443]\n",
      "epoch: 9, iter: 100, loss is: [2.4439611]\n",
      "epoch: 9, iter: 120, loss is: [2.4576595]\n",
      "epoch: 9, iter: 140, loss is: [2.4133897]\n",
      "epoch: 9, iter: 160, loss is: [2.412084]\n",
      "epoch: 9, iter: 180, loss is: [2.3829124]\n",
      "epoch: 9, iter: 200, loss is: [2.3920548]\n",
      "epoch: 9, iter: 220, loss is: [2.3614676]\n",
      "epoch: 9, iter: 240, loss is: [2.4382238]\n",
      "epoch: 9, iter: 260, loss is: [2.4266982]\n",
      "epoch: 9, iter: 280, loss is: [2.386842]\n",
      "epoch: 9, iter: 300, loss is: [2.3909838]\n"
     ]
    }
   ],
   "source": [
    "with dygraph.guard(fluid.CPUPlace()):\r\n",
    "\r\n",
    "    EPOCH_NUM = 10   \r\n",
    "    BATCH_SIZE = 7000\r\n",
    "    \r\n",
    "    # 定义外层循环\r\n",
    "    for epoch_id in rangeZ(EPOCH_NUM):\r\n",
    "        # 在每轮迭代开始之前，将训练数据的顺序随机的打乱\r\n",
    "        np.random.shuffle(training_data)\r\n",
    "        # 将训练数据进行拆分\r\n",
    "        mini_batches = [training_data[k:k+BATCH_SIZE] for k in range(0, len(training_data), BATCH_SIZE)]\r\n",
    "        \r\n",
    "        # 定义内层循环\r\n",
    "        for iter_id, mini_batch in enumerate(mini_batches):\r\n",
    "            x = np.array(mini_batch[:, :-1]).astype('float32') # 获得当前批次训练数据\r\n",
    "            y = np.array(mini_batch[:, -1:]).astype('float32') # 获得当前批次训练标签\r\n",
    "\r\n",
    "            # 将numpy数据转为飞桨动态图variable形式\r\n",
    "            buyer_features = dygraph.to_variable(x)\r\n",
    "            result = dygraph.to_variable(y)\r\n",
    "            \r\n",
    "            # 前向计算\r\n",
    "            predicts = model(buyer_features)\r\n",
    "            #loss = fluid.layers.log_loss(predicts, result)\r\n",
    "            loss = wce_loss(predicts, result)\r\n",
    "            avg_loss = fluid.layers.mean(loss)\r\n",
    "            \r\n",
    "            # 打印训练信息\r\n",
    "            if iter_id % 20 == 0:\r\n",
    "                print(\"epoch: {}, iter: {}, loss is: {}\".format(epoch_id, iter_id, avg_loss.numpy()))\r\n",
    "                # print(predicts)\r\n",
    "     \r\n",
    "            # 反向传播\r\n",
    "            avg_loss.backward()\r\n",
    "            # 最小化loss,更新参数\r\n",
    "            opt.minimize(avg_loss)\r\n",
    "            # 清除梯度\r\n",
    "            model.clear_gradients()\r\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.4 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型参数成功保存\n"
     ]
    }
   ],
   "source": [
    "model_path = './work/model'\r\n",
    "with dygraph.guard():\r\n",
    "    fluid.save_dygraph(model.state_dict(), model_path)\r\n",
    "print(\"模型参数成功保存\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4 模型应用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 4.1 预测数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_path = './work/model'\r\n",
    "with dygraph.guard():\r\n",
    "    # 加载模型参数\r\n",
    "    model_dict, _ = fluid.load_dygraph(model_path)\r\n",
    "    model.load_dict(model_dict)\r\n",
    "    model.eval()\r\n",
    "    pre = test_data.astype('float32')\r\n",
    "    pre = dygraph.to_variable(pre)\r\n",
    "    results = model(pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 4.2 结果处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id_column = pd.read_csv('X_test.csv', usecols=['customer_id'])\r\n",
    "\r\n",
    "df_preds = pd.DataFrame(\r\n",
    "{    \"customer_id\": id_column.customer_id, \r\n",
    "    \"pred\": results.numpy().flatten()}\r\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 4.3 生成文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 读入待提交文件\r\n",
    "sub = pd.read_csv('data/data19383/submission.csv')\r\n",
    "\r\n",
    "# 合并预测结果\r\n",
    "submission = pd.merge(sub, df_preds, on='customer_id', how='left')\r\n",
    "submission.fillna(0,inplace=True)\r\n",
    "submission = submission[['customer_id','pred']]\r\n",
    "submission.rename(columns={'customer_id':'customer_id','pred':'result'}, inplace=True)\r\n",
    "\r\n",
    "# 将概率值转换为用户是否购买的标签\r\n",
    "def f(x):\r\n",
    "    if x <= 0.35:   # 调整阈值\r\n",
    "        return 0\r\n",
    "    else:\r\n",
    "        return 1\r\n",
    "    return x\r\n",
    "submission['result'] = submission['result'].map(f)\r\n",
    "\r\n",
    "# 保存结果\r\n",
    "submission.to_csv('submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
